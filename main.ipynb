{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8818703,"sourceType":"datasetVersion","datasetId":5305215},{"sourceId":8824122,"sourceType":"datasetVersion","datasetId":5308816}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install google-cloud-vision\n!pip install google-cloud-core\n!pip install PyMuPDF\n\n!pip install langchain langchain-community langchain-huggingface sentencepiece langchain_chroma\n# !pip install pinecone-client\n!pip install -qU langchain-groq\n\n!pip install streamlit\n!npm install localtunnel@2.0.2","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:47:36.744246Z","iopub.execute_input":"2024-07-21T19:47:36.745149Z","iopub.status.idle":"2024-07-21T19:49:34.214454Z","shell.execute_reply.started":"2024-07-21T19:47:36.745106Z","shell.execute_reply":"2024-07-21T19:49:34.213401Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: google-cloud-vision in /opt/conda/lib/python3.10/site-packages (2.8.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.11.1)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-vision) (1.23.0)\nRequirement already satisfied: protobuf<4.0.0dev,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-vision) (3.20.3)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.62.0)\nRequirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.26.1)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2.32.3)\nRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.59.3)\nRequirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.48.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (4.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (2024.2.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-vision) (0.5.1)\nRequirement already satisfied: google-cloud-core in /opt/conda/lib/python3.10/site-packages (2.4.1)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6 in /opt/conda/lib/python3.10/site-packages (from google-cloud-core) (2.11.1)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-core) (2.26.1)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.62.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.20.3)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2.32.3)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-core) (4.9)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-core) (0.5.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.6->google-cloud-core) (2024.2.2)\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting PyMuPDFb==1.24.6 (from PyMuPDF)\n  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\nDownloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\nSuccessfully installed PyMuPDF-1.24.7 PyMuPDFb-1.24.6\nCollecting langchain\n  Downloading langchain-0.2.10-py3-none-any.whl.metadata (6.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\nCollecting langchain-huggingface\n  Downloading langchain_huggingface-0.0.3-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nCollecting langchain_chroma\n  Downloading langchain_chroma-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.3.0,>=0.2.22 (from langchain)\n  Downloading langchain_core-0.2.22-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.93-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.6)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.23.2)\nCollecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.19.1)\nRequirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (4.41.2)\nCollecting chromadb<0.6.0,>=0.4.0 (from langchain_chroma)\n  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: fastapi<1,>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from langchain_chroma) (0.108.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nCollecting build>=1.0.3 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\nCollecting chroma-hnswlib==0.7.5 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9.0)\nCollecting onnxruntime>=1.14.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nCollecting pypika>=0.48.9 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.66.4)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.1.1)\nRequirement already satisfied: grpcio>=1.58.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.59.3)\nCollecting bcrypt>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting mmh3>=4.0.1 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting orjson>=3.9.12 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.27.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1,>=0.95.2->langchain_chroma) (0.32.0.post1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.3.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (21.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3.0,>=0.2.22->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.23.0->langchain-huggingface)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.11.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.5.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2023.12.25)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.3)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.0.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.22->langchain) (2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.12.1)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.46b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (8.1.7)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (12.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.17.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.5.1)\nDownloading langchain-0.2.10-py3-none-any.whl (990 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.2.9-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\nDownloading langchain_chroma-0.1.2-py3-none-any.whl (9.3 kB)\nDownloading chromadb-0.5.4-py3-none-any.whl (581 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.4/581.4 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.2.22-py3-none-any.whl (373 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.1-py3-none-any.whl (21 kB)\nDownloading kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=818237c586f55842fe961363a78c3892ed2583420fba020bcd83c8fac178689e\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, mmh3, pyproject_hooks, packaging, orjson, opentelemetry-util-http, humanfriendly, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, opentelemetry-instrumentation-asgi, langchain-core, sentence-transformers, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-huggingface, langchain, chromadb, langchain-community, langchain_chroma\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.1.3 build-1.2.1 chroma-hnswlib-0.7.5 chromadb-0.5.4 coloredlogs-15.0.1 humanfriendly-10.0 kubernetes-30.1.0 langchain-0.2.10 langchain-community-0.2.9 langchain-core-0.2.22 langchain-huggingface-0.0.3 langchain-text-splitters-0.2.2 langchain_chroma-0.1.2 langsmith-0.1.93 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.18.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.10.6 packaging-24.1 posthog-3.5.0 pypika-0.48.9 pyproject_hooks-1.1.0 sentence-transformers-3.0.1\nCollecting streamlit\n  Downloading streamlit-1.36.0-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.3.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.8.2)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (24.1)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.2.1)\nRequirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.5.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (14.0.2)\nRequirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.32.3)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (13.7.0)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.2.3)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.9.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.3.3)\nCollecting watchdog<5,>=2.1.5 (from streamlit)\n  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\nDownloading streamlit-1.36.0-py2.py3-none-any.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.36.0 watchdog-4.0.1\n\u001b[K\u001b[?25hm##################\u001b[0m] / reify:axios: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.npmjs.o\u001b[0m\u001b[Kpmjs.o\u001b[0m\u001b[K\nadded 22 packages in 2s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.1.0\u001b[39m -> \u001b[32m10.8.2\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.8.2\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.8.2\u001b[39m to update!\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import fitz\n\nfrom PIL import Image\n\nimport cv2\nimport numpy as np\nimport os\n\nfrom google.auth import exceptions\nfrom google.oauth2 import service_account\nfrom google.cloud import vision\n\nimport io\nimport re\n\nfrom langchain.prompts import PromptTemplate","metadata":{"execution":{"iopub.status.busy":"2024-07-07T18:39:17.124159Z","iopub.execute_input":"2024-07-07T18:39:17.125058Z","iopub.status.idle":"2024-07-07T18:39:22.207166Z","shell.execute_reply.started":"2024-07-07T18:39:17.125022Z","shell.execute_reply":"2024-07-07T18:39:22.206227Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_columns_mask(image):\n    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # vert_kernel = np.ones((17,1))\n    # vertical = cv2.dilate(gray_scale, vert_kernel, iterations=1)\n    # vertical = cv2.dilate(vertical, np.ones((15,2)), iterations=1)\n    # vertical = cv2.erode(vertical, np.ones((15,5)), iterations=2)\n    vert_kernel = np.ones((17,1))\n    vertical = cv2.dilate(gray_scale, vert_kernel, iterations=1)\n    vertical = cv2.dilate(vertical, np.ones((15,1)), iterations=1)\n    vertical = cv2.erode(vertical, np.ones((15,3)), iterations=7)\n    _, vertical = cv2.threshold(vertical, 250, 255, cv2.THRESH_BINARY_INV)\n    \n    contours, _ = cv2.findContours(vertical, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n    img_copy = np.zeros_like(vertical)\n\n    vertical_lines = []\n\n    for contour in contours:\n        p = cv2.arcLength(contour, False)\n        \n        if p/2 > 500:\n            img_copy = cv2.drawContours(img_copy, contour, -1, (255,255,255), 2)\n            \n            vertical_lines.append(contour)\n    \n    if len(vertical_lines) == 0 or len(vertical_lines) == 1:\n        return None\n    \n    sorted_lines = sorted(vertical_lines, key=lambda x: x.mean(axis=0)[0][0])\n    prev_x = 0\n\n    masks = []\n\n    for line in sorted_lines:\n        max_x, max_y = line.max(axis=0)[0]\n        mean_x = line.mean(axis=0)[0][0]\n        \n        _, min_y = line.min(axis=0)[0]\n\n        if max_x - prev_x > 200:\n            # mask = np.zeros_like(img_copy)\n            mask = np.zeros_like(image)\n            mask[min_y:max_y, prev_x:max_x, :] = 1\n            \n            masks.append(mask)\n        prev_x = int(mean_x)\n    \n    return masks\n\n\ndef get_tables(img):\n    \n    # print(img.shape)\n    \n    masks = get_columns_mask(img)\n    \n    if masks is None:\n        return [img]\n    \n    images = []\n    \n    for mask in masks:\n        images.append((img * mask).astype(np.uint8))\n        \n    \n    for mask in masks:\n        img = img * (mask == 0).astype(np.uint8)\n    \n    return img, *images","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:31:30.204371Z","iopub.execute_input":"2024-07-07T17:31:30.204821Z","iopub.status.idle":"2024-07-07T17:31:30.222554Z","shell.execute_reply.started":"2024-07-07T17:31:30.204787Z","shell.execute_reply":"2024-07-07T17:31:30.220530Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"code","source":"def pil_to_cv2(image):\n    # Convert PIL image to NumPy (RGB)\n    image_np = np.array(image.convert('RGB'))\n    \n    # Convert RGB to BGR (OpenCV uses BGR order)\n    image_np = image_np[:, :, ::-1].copy()\n    \n    return image_np\n\n\ndef resize_and_pad_images(images):\n    # Step 1: Determine max height and width\n    max_width = 0\n    max_height = 0\n\n    # Find maximum dimensions among all images\n    for img in images:\n        height, width = img.shape[:2]  # Get image height and width\n        if width > max_width:\n            max_width = width\n        if height > max_height:\n            max_height = height\n\n    # Step 2: Resize and pad images\n    padded_images = []\n    for img in images:\n        height, width = img.shape[:2]\n        if width < max_width or height < max_height:\n            # Calculate padding\n            # top = (max_height - height) // 2\n            top = 0\n            # bottom = max_height - height - top\n            bottom = 0\n            left = (max_width - width) // 2\n            right = max_width - width - left\n            # Pad the image\n            padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n            padded_images.append(padded_img)\n        else:\n            padded_images.append(img)\n\n    return padded_images","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:31:30.377028Z","iopub.execute_input":"2024-07-07T17:31:30.377526Z","iopub.status.idle":"2024-07-07T17:31:30.391044Z","shell.execute_reply.started":"2024-07-07T17:31:30.377484Z","shell.execute_reply":"2024-07-07T17:31:30.389556Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"%%writefile ocr.py\n\nfrom google.auth import exceptions\nfrom google.oauth2 import service_account\nfrom google.cloud import vision\nimport io\nimport cv2\nimport os\nimport numpy as np\n\nclass OCR:\n    \n    def __init__(self):\n        self.__load_api_key()        \n        self.client = vision.ImageAnnotatorClient()\n\n    def __load_api_key(self):        \n        # Set the environment variable\n        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/project/google_vision_service.json'\n\n        # Verify the environment variable\n        credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n        print(f\"GOOGLE_APPLICATION_CREDENTIALS={credentials_path}\")\n\n        try:\n            # Load the credentials\n            credentials = service_account.Credentials.from_service_account_file(credentials_path)\n            print(\"Credentials loaded successfully\")\n        except FileNotFoundError:\n            print(f\"The file {credentials_path} was not found.\")\n        except exceptions.DefaultCredentialsError as e:\n            print(f\"An error occurred: {e}\")\n        \n    def detect_text_from_image(self, image: np.ndarray):\n        \n        # convert to gray scale and use adaptive thresholding\n        # gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        # scanned_image = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n        #                                     cv2.THRESH_BINARY, 11, 2)\n\n        cv2.imwrite(\"./temp.png\", image)\n        \n        with io.open(\"./temp.png\", \"rb\") as img_file:\n            content = img_file.read()\n    \n        vision_image = vision.Image(content=content)\n        response = self.client.text_detection(image=vision_image)\n        \n        os.remove(\"./temp.png\")\n            \n        return response.full_text_annotation.text\n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:49:38.849579Z","iopub.execute_input":"2024-07-21T19:49:38.850302Z","iopub.status.idle":"2024-07-21T19:49:38.858531Z","shell.execute_reply.started":"2024-07-21T19:49:38.850261Z","shell.execute_reply":"2024-07-21T19:49:38.857629Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing ocr.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile document.py\n\nimport fitz\nfrom PIL import Image\nimport numpy as np\n\n\ndef pil_to_cv2(image):\n    # Convert PIL image to NumPy (RGB)\n    image_np = np.array(image.convert('RGB'))\n    \n    # Convert RGB to BGR (OpenCV uses BGR order)\n    image_np = image_np[:, :, ::-1].copy()\n    \n    return image_np\n\n\nclass Document:\n    def __init__(self):\n        self.document_path = None\n        self.document = None\n        self.num_pages = None\n        \n    def load_document(self, document_path):\n        try:\n            self.document_path = document_path\n            self._open_document()\n            self.num_pages = len(self.document)\n            print(\"Document Loaded\")\n            self._close_document()\n            \n        except Exception as e:\n            print(e)\n            \n    def get_page(self, page_num):    \n        if page_num >= self.num_pages or page_num < -1:\n            print( \"Page number out of range\")\n            return\n        \n        if page_num == -1:\n            page_num = self.num_pages - 1\n        \n        self._open_document()\n        page = self.document.load_page(page_num)\n        # Render the page as an image (RGBA)\n        pix = page.get_pixmap()\n        \n        image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n        self._close_document()\n\n        return pil_to_cv2(image)\n    \n    def get_all_pages(self):\n        pages = []\n        \n        for i in range(self.num_pages):\n            pages.append(self.get_page(i))\n        \n        return pages\n        \n    def _open_document(self):\n        self.document = fitz.open(self.document_path)\n\n    def _close_document(self):\n        self.document.close()\n    \n    def __len__(self):\n        return self.num_pages\n    \n    \n    def __getitem__(self, idx):\n        if isinstance(idx, slice):\n            return [self.get_page(i) for i in range(*idx.indices(self.num_pages))]\n        elif isinstance(idx, int):\n            return self.get_page(idx)\n        else:\n            raise TypeError(\"Invalid argument type. Must be int or slice.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:49:39.116245Z","iopub.execute_input":"2024-07-21T19:49:39.116541Z","iopub.status.idle":"2024-07-21T19:49:39.123188Z","shell.execute_reply.started":"2024-07-21T19:49:39.116517Z","shell.execute_reply":"2024-07-21T19:49:39.122295Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing document.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile document_reader.py\n\nimport numpy as np\nimport re\nfrom langchain.prompts import PromptTemplate\nimport cv2\n\ndef get_columns_mask(image):\n    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    vert_kernel = np.ones((17,1))\n    vertical = cv2.dilate(gray_scale, vert_kernel, iterations=1)\n    vertical = cv2.dilate(vertical, np.ones((15,1)), iterations=1)\n    vertical = cv2.erode(vertical, np.ones((15,3)), iterations=7)\n    _, vertical = cv2.threshold(vertical, 250, 255, cv2.THRESH_BINARY_INV)\n\n    contours, _ = cv2.findContours(vertical, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n\n    img_copy = np.zeros_like(vertical)\n\n\n    vertical_lines = []\n\n    for contour in contours:\n        p = cv2.arcLength(contour, False)\n        \n        if p/2 > 200:\n            img_copy = cv2.drawContours(img_copy, contour, -1, (255,255,255), 2)\n            \n            vertical_lines.append(contour)\n\n    sorted_lines = sorted(vertical_lines, key=lambda x: x.mean(axis=0)[0][0])\n    prev_x = 0\n\n    masks = []\n\n    for line in sorted_lines:\n        max_x, max_y = line.max(axis=0)[0]\n        mean_x = line.mean(axis=0)[0][0]\n        \n        min_x, min_y = line.min(axis=0)[0]\n\n        if max_x - prev_x > 200:\n            mask = np.zeros_like(image)\n            \n            mask[min_y:max_y, prev_x:max_x,:] = 1\n            \n            masks.append(mask)\n        prev_x = int(min_x)\n    \n    return masks\n    \n\ndef get_tables(img):\n    \n    # print(img.shape)\n    \n    masks = get_columns_mask(img)\n    \n    if masks is None:\n        return [img]\n    \n    images = []\n    \n    for mask in masks:\n        images.append((img * mask).astype(np.uint8))\n        \n    \n    for mask in masks:\n        img = img * (mask == 0).astype(np.uint8)\n    \n    return img, *images\n\ndef resize_and_pad_images(images):\n    # Step 1: Determine max height and width\n    max_width = 0\n    max_height = 0\n\n    # Find maximum dimensions among all images\n    for img in images:\n        height, width = img.shape[:2]  # Get image height and width\n        if width > max_width:\n            max_width = width\n        if height > max_height:\n            max_height = height\n\n    # Step 2: Resize and pad images\n    padded_images = []\n    for img in images:\n        height, width = img.shape[:2]\n        if width < max_width or height < max_height:\n            # Calculate padding\n            # top = (max_height - height) // 2\n            top = 0\n            # bottom = max_height - height - top\n            bottom = 0\n            left = (max_width - width) // 2\n            right = max_width - width - left\n            # Pad the image\n            padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n            padded_images.append(padded_img)\n        else:\n            padded_images.append(img)\n\n    return padded_images\n\nclass DocumentReader:\n    \n    def __init__(self, document, ocr):    \n        self.document = document\n        self.ocr = ocr\n        self.text = None\n        # self._read_document()\n        \n    def _read_document(self, llm=None):\n        \n        max_page_len = 3\n        \n        self.text = \"\"\n        \n        all_pages = self.document.get_all_pages()\n        temp_pages = []\n        \n        for page in all_pages:\n            for p in get_tables(page):\n                temp_pages.append(p)\n            \n        if len(all_pages) == len(temp_pages):\n            print(\"no tables found\")\n        \n        self.all_pages = temp_pages\n        doc_len = len(self.all_pages)\n        \n        if doc_len < max_page_len:\n            combined_doc = np.vstack(resize_and_pad_images(self.all_pages))\n            detected_text = self.ocr.detect_text_from_image(combined_doc)\n            print(detected_text)\n            print(\"******************************************\")\n            self.text = self._post_process_text(detected_text, llm)\n        else:\n            for i in range(int(np.ceil(doc_len / max_page_len))):\n                pages = self.all_pages[i*max_page_len: (i+1)*max_page_len]\n                combined_doc = np.vstack(resize_and_pad_images(pages))\n                detected_text = self.ocr.detect_text_from_image(combined_doc)\n                print(detected_text)\n                print(\"******************************************\")\n                processed_text = self._post_process_text(detected_text, llm)\n                self.text += \"\\n\" + processed_text\n    \n#     def _post_process_text(self, text):\n#         # separate by .\n#         pattern = r'\\.(?!\\d)'\n#         # separate by . and :\n#         # pattern = r'(?<!\\d)(?<!\\d:)[.:](?!\\d)'\n\n#         result = re.split(pattern, text.replace(\"\\n\", \" \"))\n\n#         return \".\\n\".join([text for text in result if text])\n    \n    def _post_process_text(self, text, llm):\n        # separate by .\n        pattern = r'\\.(?!\\d)'\n        # separate by . and :\n        # pattern = r'(?<!\\d)(?<!\\d:)[.:](?!\\d)'\n\n        result = re.split(pattern, text.replace(\"\\n\", \" \"))\n        result = \".\\n\".join([text for text in result if text])\n        \n        if llm is None:\n            return result\n        \n        correction_template = \"\"\"\nInside the space dilimited by --- is a text that has some spelling mistakes and some words may be repeated.\nYour task is to correct the spelling mistakes, remove the repeated words, seperate paragraphs when the context changes and add fullstops and commas in necessary places. But do not give headings to paragraphs leave it as it is.\nDo not make any changes that are not mentioned above.\n---{context}---\n\"\"\"\n\n        correction_prompt = PromptTemplate.from_template(template=correction_template)\n        correction_chain = correction_prompt | llm\n        \n        response = correction_chain.invoke({\"context\":result})\n        \n        return \"\\n\".join(response.content.split(\"\\n\")[1:])\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:49:39.495189Z","iopub.execute_input":"2024-07-21T19:49:39.495467Z","iopub.status.idle":"2024-07-21T19:49:39.504521Z","shell.execute_reply.started":"2024-07-21T19:49:39.495444Z","shell.execute_reply":"2024-07-21T19:49:39.503614Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing document_reader.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile vector_store.py\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\nfrom langchain_chroma import Chroma\n\nclass VectorStore:\n    def __init__(self):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=2000,\n            chunk_overlap=500,\n            length_function=len,\n            is_separator_regex=False,\n        )\n        \n        self.embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n        self.vectorstore = None\n    \n    def split_text_and_create_vector_db(self, text):\n        texts = self.text_splitter.split_text(text)\n        self.text_splits = self.text_splitter.create_documents(texts)\n        \n        self.vectorstore = Chroma.from_documents(documents=self.text_splits, embedding=self.embedding_function)\n        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\":5})\n#         self.retriever = self.vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\":5, 'score_threshold': 0.0})\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:49:39.868658Z","iopub.execute_input":"2024-07-21T19:49:39.869378Z","iopub.status.idle":"2024-07-21T19:49:39.875088Z","shell.execute_reply.started":"2024-07-21T19:49:39.869351Z","shell.execute_reply":"2024-07-21T19:49:39.874205Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing vector_store.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile generator.py\n\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_groq import ChatGroq\nfrom langchain_core.prompts import MessagesPlaceholder\nfrom langchain.schema import SystemMessage, HumanMessage, AIMessage\n\nclass Generator:\n    \n    def __init__(self, retriever):\n        \n        self.llm = ChatGroq(model=\"llama3-70b-8192\")\n        self.retriever = retriever\n        \n        self.system_prompt = \"\"\"Use the following pieces of context to answer the question at the end.\nOnly use the information present in the context to answer the question.\nEven if you know the answer, but if is not present in the given context do not answer the question.\nReturn the answer as it is in the document and do not try to summarize or expand the topic.\nDo not make things up that are otherwise not in the context.\nIf answer is not present in the context, just say that you don't know, don't try to make up an answer.\nIf you don't understand the question just say that you didn't the question and to repeat the question.\nAlways say \"Thanks for asking!\" at the end of the answer. \n{context}\n\"\"\"\n\n        self.prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", self.system_prompt),\n                MessagesPlaceholder(\"chat_history\"),\n                (\"human\", \"{input}\"),\n            ]\n        )\n        \n        self.chat_history = []\n        \n        self.question_answer_chain = create_stuff_documents_chain(self.llm, self.prompt)\n        self.rag_chain = create_retrieval_chain(self.retriever, self.question_answer_chain)\n        \n    def generate_answer(self, query):\n        response = self.rag_chain.invoke({\"input\": query, \"chat_history\":self.chat_history})\n\n        self.chat_history.extend(\n        [\n            HumanMessage(response['input']),\n            AIMessage(\"context\"),\n        ])\n        \n        return response\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:49:40.325916Z","iopub.execute_input":"2024-07-21T19:49:40.326236Z","iopub.status.idle":"2024-07-21T19:49:40.332786Z","shell.execute_reply.started":"2024-07-21T19:49:40.326210Z","shell.execute_reply":"2024-07-21T19:49:40.331881Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing generator.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from getpass import getpass\nimport os\n\nos.environ[\"GROQ_API_KEY\"] = getpass()\n\nfrom langchain_groq import ChatGroq\n\nllm = ChatGroq(model=\"llama3-70b-8192\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:15:18.906695Z","iopub.execute_input":"2024-07-07T14:15:18.907124Z","iopub.status.idle":"2024-07-07T14:15:22.268383Z","shell.execute_reply.started":"2024-07-07T14:15:18.907090Z","shell.execute_reply":"2024-07-07T14:15:22.267148Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdin","text":" ························································\n"}]},{"cell_type":"code","source":"correction_template = \"\"\"\nInside the space dilimited by --- is a text that has some spelling mistakes and some words may be repeated.\nYour task is to correct the spelling mistakes, remove the repeated words, seperate paragraphs when the context changes and add fullstops and commas in necessary places.\nDo not make any changes that are not mentioned above.\n---{context}---\n\"\"\"\n\ncorrection_prompt = PromptTemplate.from_template(template=correction_template)\ncorrection_chain = correction_prompt | llm\n\n\n\nresponse = correction_chain.invoke({\"context\":result})","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:48:18.888825Z","iopub.execute_input":"2024-07-07T15:48:18.889389Z","iopub.status.idle":"2024-07-07T15:48:19.550975Z","shell.execute_reply.started":"2024-07-07T15:48:18.889350Z","shell.execute_reply":"2024-07-07T15:48:19.549704Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"print(response.content)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T15:48:51.064376Z","iopub.execute_input":"2024-07-07T15:48:51.064795Z","iopub.status.idle":"2024-07-07T15:48:51.072148Z","shell.execute_reply.started":"2024-07-07T15:48:51.064764Z","shell.execute_reply":"2024-07-07T15:48:51.070407Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Here is the corrected text:\n\n**State Mathematical Model**\n\nA static mathematical model describes a system at a specific point in time, without considering its behaviour over time. The variables are state and do not change with time. This is suitable for systems with constant or unchanging parameters. It represents a system state at a particular instant.\n\n**Dynamic Mathematical Model**\n\nA dynamic mathematical model captures the behaviour of a system over time, incorporating time-dependent relationships. The variables are time-dependent and change over time. It is suitable for systems with changing parameters and dynamic behaviour. It represents the dynamic behaviour of the system.\n","output_type":"stream"}]},{"cell_type":"code","source":"ocr = OCR()","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:18:20.376585Z","iopub.execute_input":"2024-07-07T17:18:20.377121Z","iopub.status.idle":"2024-07-07T17:18:20.494391Z","shell.execute_reply.started":"2024-07-07T17:18:20.377081Z","shell.execute_reply":"2024-07-07T17:18:20.493161Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"GOOGLE_APPLICATION_CREDENTIALS=/kaggle/input/project/google_vision_service.json\nCredentials loaded successfully\n","output_type":"stream"}]},{"cell_type":"code","source":"document_path = \"/kaggle/input/pdf-project/handwritten.pdf\"\n\ndoc = Document()\ndoc.load_document(document_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:18:21.957233Z","iopub.execute_input":"2024-07-07T17:18:21.958245Z","iopub.status.idle":"2024-07-07T17:18:21.967387Z","shell.execute_reply.started":"2024-07-07T17:18:21.958170Z","shell.execute_reply":"2024-07-07T17:18:21.966243Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"Document Loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"doc_reader = DocumentReader(doc, ocr)\ndoc_reader._read_document(llm=llm)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:18:22.128741Z","iopub.execute_input":"2024-07-07T17:18:22.129215Z","iopub.status.idle":"2024-07-07T17:18:29.804421Z","shell.execute_reply.started":"2024-07-07T17:18:22.129156Z","shell.execute_reply":"2024-07-07T17:18:29.802927Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"no tables found\n","output_type":"stream"}]},{"cell_type":"code","source":"vec = VectorStore()\nvec.split_text_and_create_vector_db(doc_reader.text)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:22:45.935410Z","iopub.execute_input":"2024-07-07T14:22:45.935857Z","iopub.status.idle":"2024-07-07T14:22:48.528475Z","shell.execute_reply.started":"2024-07-07T14:22:45.935825Z","shell.execute_reply":"2024-07-07T14:22:48.527266Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"generator = Generator(vec.retriever)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:22:52.229709Z","iopub.execute_input":"2024-07-07T14:22:52.230167Z","iopub.status.idle":"2024-07-07T14:22:52.261830Z","shell.execute_reply.started":"2024-07-07T14:22:52.230134Z","shell.execute_reply":"2024-07-07T14:22:52.260499Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"response = generator.generate_answer(\"Who are you?\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:24:56.585498Z","iopub.execute_input":"2024-07-07T14:24:56.585930Z","iopub.status.idle":"2024-07-07T14:24:57.345680Z","shell.execute_reply.started":"2024-07-07T14:24:56.585897Z","shell.execute_reply":"2024-07-07T14:24:57.344456Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"print(response['answer'])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T14:24:58.397921Z","iopub.execute_input":"2024-07-07T14:24:58.398389Z","iopub.status.idle":"2024-07-07T14:24:58.404781Z","shell.execute_reply.started":"2024-07-07T14:24:58.398351Z","shell.execute_reply":"2024-07-07T14:24:58.403466Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"I'm an AI designed to answer questions based on the provided context. I don't have personal information or identity. I exist solely to assist and provide information within the scope of the given context. Thanks for asking!\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"\\n\\n\\n\\n\".join(x.page_content for x in response['context']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.schema import SystemMessage, HumanMessage, AIMessage\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\nfrom langchain_chroma import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:19:11.006722Z","iopub.execute_input":"2024-07-07T17:19:11.007928Z","iopub.status.idle":"2024-07-07T17:19:11.124681Z","shell.execute_reply.started":"2024-07-07T17:19:11.007874Z","shell.execute_reply":"2024-07-07T17:19:11.122202Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"HUGGINGFACEHUB_API_TOKEN = getpass()\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.419545Z","iopub.status.idle":"2024-07-07T08:07:02.420046Z","shell.execute_reply.started":"2024-07-07T08:07:02.419788Z","shell.execute_reply":"2024-07-07T08:07:02.419805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# llm = HuggingFaceEndpoint(\n#     repo_id=model_name,\n#     temperature=0.001,\n#     top_p = 0.99,\n#     repetition_penalty = 1.2,\n# )\n\n\n# os.environ[\"GROQ_API_KEY\"] = getpass()\n\n# from langchain_groq import ChatGroq\n\n# llm = ChatGroq(model=\"llama3-70b-8192\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.421807Z","iopub.status.idle":"2024-07-07T08:07:02.422546Z","shell.execute_reply.started":"2024-07-07T08:07:02.422274Z","shell.execute_reply":"2024-07-07T08:07:02.422303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=500,\n    length_function=len,\n    is_separator_regex=False,\n)\n\ntexts = text_splitter.split_text(doc_reader.text)\ntexts = text_splitter.create_documents(texts)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:19:14.664711Z","iopub.execute_input":"2024-07-07T17:19:14.665136Z","iopub.status.idle":"2024-07-07T17:19:14.672992Z","shell.execute_reply.started":"2024-07-07T17:19:14.665104Z","shell.execute_reply":"2024-07-07T17:19:14.671471Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"# embeddings_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\nembedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# embeddings = HuggingFaceEmbeddings(model_name=embeddings_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:19:17.408394Z","iopub.execute_input":"2024-07-07T17:19:17.408855Z","iopub.status.idle":"2024-07-07T17:19:18.555599Z","shell.execute_reply.started":"2024-07-07T17:19:17.408820Z","shell.execute_reply":"2024-07-07T17:19:18.554266Z"},"trusted":true},"execution_count":161,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate, ChatPromptTemplate\nfrom langchain_core.prompts import MessagesPlaceholder\n\nsystem_prompt = \"\"\"Use the following pieces of context to answer the question at the end.\nOnly use the information present in the context to answer the question.\nReturn the answer as it is in the document and do not try to summarize or expand the topic.\nDo not make things up that are otherwise not in the context.\nIf answer is not present in the context, just say that you don't know, don't try to make up an answer.\nIf you don't understand the question just say that you didn't the question and to repeat the question.\nAlways say \"Thanks for asking!\" at the end of the answer. \n{context}\n\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:19:27.085471Z","iopub.execute_input":"2024-07-07T17:19:27.085919Z","iopub.status.idle":"2024-07-07T17:19:27.094196Z","shell.execute_reply.started":"2024-07-07T17:19:27.085886Z","shell.execute_reply":"2024-07-07T17:19:27.092912Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"code","source":"# vectorstore = Chroma.from_documents(documents=texts, embedding=embedding_function)\nretriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\":3, 'score_threshold': 0.0})","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:30:03.645637Z","iopub.execute_input":"2024-07-07T17:30:03.646068Z","iopub.status.idle":"2024-07-07T17:30:03.653857Z","shell.execute_reply.started":"2024-07-07T17:30:03.646037Z","shell.execute_reply":"2024-07-07T17:30:03.652483Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"# vectorstore.similarity_search_with_relevance_scores(\"I am a data scientist. how can in help in big data\")\nretriever.invoke(\"Characterstics of big data\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T17:30:34.779591Z","iopub.execute_input":"2024-07-07T17:30:34.780049Z","iopub.status.idle":"2024-07-07T17:30:34.811344Z","shell.execute_reply.started":"2024-07-07T17:30:34.780017Z","shell.execute_reply":"2024-07-07T17:30:34.810033Z"},"trusted":true},"execution_count":200,"outputs":[{"execution_count":200,"output_type":"execute_result","data":{"text/plain":"[Document(page_content='Big Data is a field that systematically extracts information from data, which is too large or complex to be dealt with by traditional data-processing software. Currently, big data refers to the use of predictive behavior analytics methods that extract value from the data. This can find new correlations, spot business trends, prevent diseases, combat crimes, and more.\\n\\nCharacteristics of Big Data are:\\n\\ni) Volume: This refers to the sheer amount of data being generated. The amount can reach terabytes, petabytes, or even exabytes, from social media, sensors, financial transactions, and more. Real-world Example: The sheer volume of customer interactions across millions of products creates a massive dataset.'),\n Document(page_content=\"Characteristics of Big Data are:\\n\\ni) Volume: This refers to the sheer amount of data being generated. The amount can reach terabytes, petabytes, or even exabytes, from social media, sensors, financial transactions, and more. Real-world Example: The sheer volume of customer interactions across millions of products creates a massive dataset.\\n\\nii) Velocity: Big data isn't static. It's constantly moving and growing at an exponential rate. Being able to capture and analyze this data in real-time is crucial for many applications. Example: Twitter and Facebook social media deal with a constant stream of data. New posts, tweets, direct messages, photos, and videos are created every second. This velocity of flow requires real-time analytics to track trending topics, identify crises, and personalize user feeds.\"),\n Document(page_content=\"Big Data is a field that systematically extracts information from data. It deals with large or complex data that is too difficult to be handled by traditional data-processing software.\\n\\nCurrently, big data refers to the use of predictive behavior analytics methods that extract value from the data. This can find new correlations, spot business trends, prevent diseases, combat crimes, and more.\\n\\nCharacteristics of Big Data:\\n\\ni) Volume: This refers to the sheer amount of data being generated. The amount can reach terabytes, petabytes, or even more from social media, sensors, financial data, transactions, and more.\\n\\nReal-world Example: The sheer volume of customer interactions across millions of products creates a massive dataset.\\n\\nii) Velocity: Big data isn't static. It's constantly moving and growing at an exponential rate. Being able to capture and analyze this data in real-time is crucial for many applications.\")]"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.434294Z","iopub.status.idle":"2024-07-07T08:07:02.434706Z","shell.execute_reply.started":"2024-07-07T08:07:02.434521Z","shell.execute_reply":"2024-07-07T08:07:02.434536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chat_history = []","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.436872Z","iopub.status.idle":"2024-07-07T08:07:02.437706Z","shell.execute_reply.started":"2024-07-07T08:07:02.437283Z","shell.execute_reply":"2024-07-07T08:07:02.437308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"What are the characterstics of big data?\"\n\nresponse = rag_chain.invoke({\"input\": query, \"chat_history\":chat_history})\n\nchat_history.extend(\n[\n    HumanMessage(response['input']),\n    AIMessage(\"context\"),\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.444716Z","iopub.status.idle":"2024-07-07T08:07:02.445212Z","shell.execute_reply.started":"2024-07-07T08:07:02.445004Z","shell.execute_reply":"2024-07-07T08:07:02.445022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(response['answer'], \"\\n\\n\")\nprint(*[x.page_content for x in response['context']])","metadata":{"execution":{"iopub.status.busy":"2024-07-07T08:07:02.447749Z","iopub.status.idle":"2024-07-07T08:07:02.448284Z","shell.execute_reply.started":"2024-07-07T08:07:02.448041Z","shell.execute_reply":"2024-07-07T08:07:02.448090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install streamlit\n# !npm install localtunnel@2.0.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile app.py\n\nimport streamlit as st\nfrom langchain_groq import ChatGroq\nfrom generator import Generator\nfrom document import Document\nfrom document_reader import DocumentReader\nfrom vector_store import VectorStore\nfrom ocr import OCR\nfrom langchain_groq import ChatGroq\nimport os\n\nst.title(\"PDF Bot\")\n\nif 'step' not in st.session_state:\n    st.session_state.step = 0\n\nif 'ocr' not in st.session_state:\n    with st.spinner('Initializing...'):\n        st.session_state.file_loaded = False\n        os.environ[\"GROQ_API_KEY\"] = \"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\"\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n#         llm = ChatGroq(model=\"llama3-70b-8192\")\n        st.session_state.ocr = OCR()\n        st.session_state.doc = Document()\n#         document_path = \"/kaggle/input/pdf-project/handwritten.pdf\"\n#         st.session_state.doc.load_document(document_path)\n        st.session_state.doc_reader = DocumentReader(st.session_state.doc, st.session_state.ocr)\n#         st.session_state.doc_reader._read_document(llm=llm)\n\n        st.session_state.vec = VectorStore()\n#         st.session_state.vec.split_text_and_create_vector_db(st.session_state.doc_reader.text)\n#         st.session_state.generator = Generator(st.session_state.vec.retriever)\n\ndef create_generator(pdf_path):\n    with st.spinner(\"Reading pdf...\"):\n        llm = ChatGroq(model=\"llama3-70b-8192\")\n        st.session_state.doc.load_document(pdf_path)\n        st.session_state.doc_reader._read_document(llm=llm)\n\n        if st.session_state.vec.vectorstore is not None:\n            st.session_state.vec.vectorstore.reset_collection()\n\n        st.session_state.vec.split_text_and_create_vector_db(st.session_state.doc_reader.text)\n\n        st.session_state.generator = Generator(st.session_state.vec.retriever)\n#         print(\"ready to answer questions\")\n        \ndef pdf_changed():\n    st.session_state.file_loaded = False\n    st.session_state.messages = []\n\ndef main():\n        \n#     if st.session_state.step == 0:\n#         with st.spinner('Initializing...'):\n#             os.environ[\"GROQ_API_KEY\"] = \"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\"\n#             os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n\n#             llm = ChatGroq(model=\"llama3-70b-8192\")\n\n#             # loading and reading document        \n#             ocr = OCR()\n#             document_path = \"/kaggle/input/pdf-project/handwritten.pdf\"\n#             doc = Document()\n#             doc.load_document(document_path)\n#             doc_reader = DocumentReader(doc, ocr)\n#             doc_reader._read_document(llm=llm)\n\n#             vec = VectorStore()\n#             vec.split_text_and_create_vector_db(doc_reader.text)\n\n#             generator = Generator(vec.retriever)\n#             print(\"Ready to answer question\")\n#             st.session_state.step += 1\n        \n    st.sidebar.title(\"Upload PDF Documents\")\n    uploaded_file = st.sidebar.file_uploader(\"Upload a PDF file\", type=[\"pdf\"], on_change=pdf_changed)\n    \n    print(uploaded_file is not None)\n    \n    if uploaded_file is not None:\n        \n        if not st.session_state.file_loaded:\n            \n            pdf_path = uploaded_file.name\n            print(pdf_path)\n            pdf_path = uploaded_file.name\n            with open(pdf_path, \"wb\") as f:\n                f.write(uploaded_file.getbuffer())\n            \n            create_generator(pdf_path)\n            st.session_state.file_loaded = True\n            \n            \n        \n         # Initialize chat history\n        if \"messages\" not in st.session_state:\n            st.session_state.messages = []\n\n        # Display chat messages from history on app rerun\n        for message in st.session_state.messages:\n            with st.chat_message(message[\"role\"]):\n\n                st.markdown(message[\"content\"])\n                if message[\"role\"] == 'assistant':\n                    with st.expander(\"Context\"):\n                        st.write(message['context'])\n\n        # React to user input\n        if query := st.chat_input(\"What is up?\"):\n            # Display user message in chat message container\n            st.chat_message(\"user\").markdown(query)\n            # Add user message to chat history\n            st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n            with st.spinner('Generating Response...'):\n    #             answer = query\n    #             context = [\"fdasf s\", \"fdasfas\", \"fsdfasdf\"]\n                response = st.session_state.generator.generate_answer(query)\n                answer = response['answer']\n                context = \"\\n\\n**********************************\\n\\n\".join(x.page_content for x in response['context'])\n\n            # Display assistant response in chat message container\n            with st.chat_message(\"assistant\"):\n                st.markdown(answer)\n                with st.expander(\"Context\"):\n                    st.write(context)\n\n            # Add assistant response to chat history\n            st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer, \"context\": context})\n\n    else:\n        st.session_state.file_loaded = False\n        st.session_state.messages = []\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:50:20.713801Z","iopub.execute_input":"2024-07-21T19:50:20.714450Z","iopub.status.idle":"2024-07-21T19:50:20.723915Z","shell.execute_reply.started":"2024-07-21T19:50:20.714413Z","shell.execute_reply":"2024-07-21T19:50:20.722846Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!curl ipv4.icanhazip.com","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:50:22.644600Z","iopub.execute_input":"2024-07-21T19:50:22.644929Z","iopub.status.idle":"2024-07-21T19:50:23.688898Z","shell.execute_reply.started":"2024-07-21T19:50:22.644904Z","shell.execute_reply":"2024-07-21T19:50:23.687877Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"130.211.210.208\n","output_type":"stream"}]},{"cell_type":"code","source":"!streamlit run app.py & npx localtunnel --port 8501","metadata":{"execution":{"iopub.status.busy":"2024-07-21T19:50:23.690949Z","iopub.execute_input":"2024-07-21T19:50:23.691274Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://130.211.210.208:8501\u001b[0m\n\u001b[0m\nyour url is: https://cold-wombats-watch.loca.lt\nGOOGLE_APPLICATION_CREDENTIALS=/kaggle/input/project/google_vision_service.json\nCredentials loaded successfully\n/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  warn_deprecated(\n2024-07-21 19:50:47.500804: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-21 19:50:47.500927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-21 19:50:47.672543: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nmodules.json: 100%|████████████████████████████| 349/349 [00:00<00:00, 2.31MB/s]\nconfig_sentence_transformers.json: 100%|███████| 116/116 [00:00<00:00, 1.24MB/s]\nREADME.md: 100%|███████████████████████████| 10.7k/10.7k [00:00<00:00, 54.3MB/s]\nsentence_bert_config.json: 100%|██████████████| 53.0/53.0 [00:00<00:00, 436kB/s]\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nconfig.json: 100%|█████████████████████████████| 612/612 [00:00<00:00, 5.11MB/s]\nmodel.safetensors: 100%|████████████████████| 90.9M/90.9M [00:00<00:00, 168MB/s]\ntokenizer_config.json: 100%|███████████████████| 350/350 [00:00<00:00, 1.99MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 2.45MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 10.1MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 112/112 [00:00<00:00, 1.14MB/s]\n1_Pooling/config.json: 100%|███████████████████| 190/190 [00:00<00:00, 1.82MB/s]\nFalse\nTrue\nhandwritten.pdf\nDocument Loaded\nBig Data\nData\" is a field that treat\nsystematically extract information from data.\nlarge or complex to be dealt with\ndata-processing software.\nanalyn's\nuser\nby\nways to analyze, deal, &\nthat are too\ntraditional\nCurrently big data refers to the use of predictive\nbehaviour analytics methods that extract.\nvalve from the data. This can find new correlations to\nspot business trends, prevent diseases, combat crimes, et....\nCharacteristics of Big Data;\ni) Volume\nii) Velocity\n11) Variety\n✓ Value\niv) Verocity\nVI) Variability\ni) Volume:\nThis refers to the sheer amount of data being\ngenerated. The amount can reach terabytes, petabytes, of\nfrom social media, sensors, financial\ndata that can\ncome\ntransactions, and more.\nReal world Example: The sheer volume of customes interactions.\nacross millions of products creates massive dataset.\nii) Velocity:\nBig\ndata isn't static. It's constantly moving and\ngrowing at exponential rate. Being able to capture the and\nanalyze\n\"this data in real-time is crucial for many applications.\nExample\nTwitter / Facebook social media deal with a constant\nstream of data. New post, tweets, direct message, photos,\nvideos are created every second. This velocity of flow\nrequires real time analytics to track trending topics, identify\ncrises, and personalize user feeds.\niii) Variety:\nBig data comes in all shapes and sizes. Structured data,\nLike numbers in a spreadsheet in like table or in spreadsheet\nform, unstructured datas like text, images, audio and video.\nand there\nCan be\nmedia posts.\nExample:\nIn a\nsemi-structured data like emails or social\nself-driving car, it collects data from\nvarious sensors (LIDAR, Cameras) to pre perceive its.\nsurroundings, structured data ( distance to objects), unstructured\ndata ( images of the road), also GPS and wether informations.\niv) Verocity:\nWith so much data from different sources, ensuring its\ncan be a\nand reliability\na challange...\naccuracy.\nIncomplete, inconsistent, or even contrine cover.\naccuracy\nof data.\nbe\nData may\nVeracity is truthfulness,\nExample: In healthcare, medical records and clinical trial derta\nare crucial for research and treatment. The data sources must\nbe truthfulness and should not contain error.\nverification\ncan ensure veracity.\nDato\n• cleaning &\n1.\n✓) Value:\nUltimately the goal of big data is to extract value.\nfrom all the data and information. By analyzing\nbig data,\nCompanies and organization can gain insights into customer behaviour,\noptimize operations, identify trends, and make better decisions.\nExample: Netflix extract the viewer behavious - what they\nwatch, when they stop and what they rate. by analyzing\nNetflix personalises recommendations, identify popular genres,\ndecide what show to produce.\nand even\nChallenges of Big Data Challenges\nStorage of massive amount of data.\nii. Deciding the\ntii.\nrelevant & if the data is relevant.\nThe cost of technology infrastructure\niv. Integrating data from do various sources and types.\nV.\nReal-time analytics from various sources and format\nVi. lack of skills to analyze the data\nviis\nLack of skills to\n• will. Security of the\ndata projects\nmanage big\nuser data and information.\n******************************************\nParent Tents is Aly Dentis\nis\nBig Data\nis changing undergoing revolution and\nconstantly evolving. The coat heads of by Catch\ncurrent\nFrends captures\nthe iterative and multifaceted (mory capacts phase) ture\nof the field. Some of the bends are:\n:) Iterative Premus :\nBig data projects are no longer linear. Dato\nexploration and analyns often lead to new discoveries\nthat refine the initial questions and applications,\nii) Date variety: Modern big date solutions can handle a\nvanity of data formate, not just traditional dats.\nHij AI powered analytics: Machine learning algorithing com\nautomate truke identify hidden potterne, and gemale.\nmore accurate predictions from complex datoret.\niy) Dato visualization & Storytelling: The power of big data lies in its\ninnight. Cute voralization took translate complex data to\nclear and navals presentation. This moble effective community\nif\ngecerated in\n1) Real Time Rules: The focus is shifted from towards\ncaphotry and analyzing date on\nreal time. This tables enables faster decision-making\nfraud detection, etc.\nV₁ Data Difvary: With the amount and speed and sensitive.\ncature of clata, regulations are prompting companies to\nprioritize data govermonie and be wer trust.\nRole of Data Scietats.\nDota somatist is a professional who collects, cleans, and\nstactical\nanalyze large amounts of doto using analytow, stection and\nFor sub. They was achowed analytical tooly algorithione,\nand machine learning techniques to make predictions and decisions\nfor vast amount of date.\n(Date\nCommuning\n(Mi, stataks\nAnalytics\nData Scientist\nThe key roles and responsibilite of\nroles and responsibilites of a data suentistics are:\n1. Dota Mining Extracking weable dots.\n1) Preprocessing of Date: Meaway & organizing startured Berstructured date.\nCleaning\n2) Developing hedictive Models: They\nThey we ML code algorithms to\nfind disper insights inte detouts.\nis) Enhancing existing analytics platforms?\nThey add new features & capabilitie Nuch as NLP,\nadvow worch feature, and Al-based resonemos datos igitems.\n1) Interpreting Data:\nmeaning\nextract\nThey interport the row data & rabort valuable.\nout of it..\nIs conclusion, data scientistica help brodaden the business scope of\ninverting com.\nputing by developing and improve analys\ntechiques, and date & performance management.\nShort Notes\nA. Structured Semi-Shuctured and Unshachired Catie\nand\nStructured data refine to do that have a high degres\nof agoniation, typically stored in databas using rows & columns.\nkey advantage of structured data is its ease of quering.\nanalysis, facilitated by prediff perdefined scheme or dorta model.\nHowever, structured data may face Remitations in\naccomodating dynami 09° -evolving data types and\nrequire schora modifications to incorporate\nExamples; SQL databous, spreadsheet, etc.....\nconfine to a\nmay\nnew data attribute.\nSemi-structured date is a type of data that dogpot.\nrigid structures like structured data but has\nmay\ncontain togs, markerd,\nSome organizational properties. It\nor other mechanisms for organizing and hierarchically structuring.\nthe date Unlike structund data, this doesn't require a\nSchemo or predefined data model.\nExamples: JSON (Javascript Object Notation), XML (extensible.\nMarkup Language),\nSome NoSQL databases.\nUnstructured data refers to data that doesn't have a\npredefined data model or structor. It lacks.\nany specific\n.\norganization and doesout fit neatly into a databove or table format.\nThey are\nare wually text hoory, contoine multimedia such as images,\nvidres and audio file. Analysing then is challenging and\nregates advance tertiques.\nExample: Social media posts, web.\net...\npages,\n******************************************\nBig Date io Claud\ndolobos datasets\nBig data in cloud reface to the practice of\nstring, processing and analying the large and complex\nusing cloud computing resources, Cloud\nplatforms offer scalability, flexibility, and cost-effectiveness,\nmaking them well suited for managing massive volumes of\ndata. By leveraging cloud-based infrastructure and services,\norganizatime\ncan\nacress\non-demand computing resources, much as\nstorage and processing power, to handle by data overloade\nws\nwithout the need for significant upfront investmends in\nhardware and infrastructure.\nAdditionally, cloud providers offer a variety of managed.\nbig data services, including data lakes, analyticat anayties\nplatforms and machine learning took further simplifying the\ndevelopment and management of big date solutions.\nThe combination of big date and cloud computing exables\norganizations to derive volwable weights, improved decision-making,\nand drive innovation while maximing efficiency and minimising\noperational\noverhead.\nData Analytica\nDarke analytice is a process of collecting transforming,\nData\ncleaning, and moulding date with the good of dierovering the\nrequired information. Data visualization can be used to potrary.\nthe clots for the ease of discovering the cnful paterne in.\nuseful\nthe date.\ndata\nData analysis process consists of the following phrases that\nare iterative in nature-\n6. Donts requirement specification\nii. Data Collection\nif Duto Processing\niv. Data Cleaning\nV. Data Analysis\nInterpretation\nvi. Q\nVii.\nCommunication\nData requirement specifications &\nData Collection\n↓\n[Data Processing\n[Data Cleaning &\nT\nDota Gabi\n↓\nالعلامة\nCommunication\nHerative\nprocess\n******************************************\n1. Gathering data to be analyzed\n2. Dato Cleaning\n3. Prolying to docter what it\nAnalyzing\nmay\nreveal\n4. Interpreting it to understand the process. by which the\ndata was analyzed and to incons I've rearbed\n6. Communication about the analyzed & iterate for better analysis\nNed for Data Analytics\n1. Informed Desic Making\nii. Competitive Advantage.\nkii. Enhamed Castomer experience & retention.\niv. Operational Efficiency\n✓.\nRick Management\nvi Innovation & Growth\nvii. Regulatory Compliance\n9 How Big data can be helpful to increase business reverse. Expli\nвід.\n1. Enhanced Customer insights\nii. Improved operational effortry\n411. Targeting Marketing Campaigns.\niv. Predictive analytics\nV.\nVisualization of plane & current state of company\n******************************************\nTrue\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}